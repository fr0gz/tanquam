---
title: "Using Eigen for Matrix Operations in C++ with Data Simulation"
date: 2020-06-06
translationKey: "using-eigen-matrix-cpp"
draft : false 
math: true
---

## Summary

**Eigen** is a C++ linear algebra library that performs the heavy lifting behind the matrix operations used by TensorFlow. My motivation for writing this post is to dive deeper into the technologies that power neural network libraries, with the goal of learning how to build my own.

This becomes especially relevant with the rise of **TinyML** and **TensorFlow Lite**, which enable neural network algorithms to run on small devices without requiring an internet connection or cloud processing. This practical application has sparked my curiosity, and understanding how these systems work at a low level seems increasingly important for the near future.

## Dependencies

First, we need to load the dependencies. In this particular case, what I want to do is import tools from the Eigen library to construct vectors, as well as the matrix operation I want to perform, which is linear regression.

I also want to populate my vectors with a bunch of random data. So, I’ll need a random number generator along with the probability distributions from which I want to sample those numbers. (C++ supports a wide variety of random number generators and distributions.) For this, I’ll need to use tools from the <random> header.

In addition, I want to benchmark the performance of these operations in order to compare them with NumPy later. To do that, I’ll need to set up a clock object to measure execution time.  

{{< highlight cpp "linenos=inline, hl_lines=1, style=dracula" >}}
/******************************************************************************
  Prácticas de matrices con Eigen: Haciendo una Regresion Lineal


  Copyright 2020 - Cristián Andrés Guevara Kalil <cguevarak@pm.me>
  University of Chile
  Redistribution and use in source and binary forms, with or without
  modification, are permitted provided that the following conditions
  are met:
  1. Redistributions of source code must retain the above copyright
  notice, this list of conditions and the following disclaimer.
  2. Redistributions in binary form must reproduce the above copyright
  notice, this list of conditions and the following disclaimer in the
  documentation and/or other materials provided with the distribution.
  3. Neither the name of the copyright holder nor the names of its
  contributors may be used to endorse or promote products derived from
  this software without specific prior written permission.
  Complete BSD-3-clause License: https://opensource.org/licenses/BSD-3-Clause
******************************************************************************/

#include <iostream>
#include <time.h>
#include <random>
#include <Eigen/Dense>

{{< /highlight >}}

## Running the Simulation: Generating My Data

The first step is to generate random data. To do that, I’ll need a pseudorandom number generator — in this case, a Mersenne Twister. I’m setting the seed to 24 because it’s 42 reversed (Douglas Adams readers know what that means: so long, and thanks for all the fish). This makes the experiment reproducible.

Next, I define the normal distributions I’ll be using — one for x and one for y. To generate a standard normal distribution, you need to specify its mean and standard deviation.

{{< highlight cpp "linenos=inline, hl_lines=1, style=dracula" >}}
 	std::mt19937_64 mt_engine{ 24 };
 	std::normal_distribution<double> disx(10, 1);
	std::normal_distribution<double> disy(100, 15);
{{< /highlight >}}

## Building the Matrices

Now let’s bring in the Eigen library. Conceptually, what I want to do is create a structure to store the data that will represent my vector x and vector y. To perform this operation, the code uses modern C++ syntax, so the proper configuration is needed to ensure it compiles correctly.

Eigen's syntax is very semantic: you declare a matrix to store double-type values; a matrix to store float-type values; or, for example, a 5000×2 zero matrix that will hold the data generated by my engine and drawn from a distribution with previously defined parameters.

However, one part that deserves further reflection is how functions are applied to matrices. This is the specific way I implement the concept of taking numbers from my distribution and storing them in the matrix.

{{< highlight cpp "linenos=inline, hl_lines=1, style=dracula" >}}
    Eigen::MatrixXd x = Eigen::MatrixXf::Zero(5000,2).unaryExpr([&](double dummy){return disx(mt_engine);});
    Eigen::MatrixXd y = Eigen::MatrixXf::Zero(5000,1).unaryExpr([&](double dummy){return disy(mt_engine);});
    Eigen::MatrixXd xtx = x.transpose() * x;
    Eigen::MatrixXd xty = x.transpose() * y;
{{< /highlight >}}

After generating the data, the idea I'm trying to express is: "assign to the variable xtx the value of x transposed times x."
Similarly, the variable xty stores the result of x transposed times y.

For readers with a background in economics or finance, this is no mystery: these are the core components of the matrix formula to compute the betas in a linear regression:

\begin{aligned}
\hat{\beta} = (X' X)^{-1} (X' Y)
\end{aligned}


## Calculating the inverse and the Betas

Now, to compute the inverse, Eigen provides a quite semantic function.

{{< highlight cpp "linenos=inline, hl_lines=1, style=dracula" >}}
   std::cout << "Inversa de X'*X" << std::endl;
   std::cout << xtx.inverse() << std::endl;
   std::cout << "Betas: " << std::endl;
   std::cout << xtx.inverse() * xty << std::endl;
{{< /highlight >}}

## Performing the Benchmarking

To do the benchmarking, what I did was wrap my little program from the beginning with a timer as follows:

{{< highlight cpp "linenos=inline, hl_lines=1, style=dracula" >}}
  clock_t begin = clock();

  double elapsed_secs = double(end - begin) / CLOCKS_PER_SEC;
  std::cout << "Time taken : " << elapsed_secs << std::endl;
{{< /highlight >}}

 ## Finally:

 The applications of the Eigen library are quite interesting, and I must say that according to my comparison with similar operations in NumPy, in my case at least, it was quite fast (around 2.43 vs 12 seconds). As a thank you, I want to express my gratitude to the open-source community, which made the successful implementation of this application possible.
 
 
